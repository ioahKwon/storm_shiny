{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Dask for multi-processing.\n",
    "\n",
    "This notebooks shows how you might use Dask to run 3D-DAOSTORM in parallel on a single movie. The idea is similar to the approach for SLURM. We're going to break up the analysis by creating XML files for sub-sets of the frames in the movie. Then we'll run the analysis in parallel on each of the sub-sets and finally combine the results into a single HDF5 file.\n",
    "\n",
    "At least for this simple example movie, and on my laptop, this is not actually any faster than doing the analysis serially, but it at least demonstrates the idea.\n",
    "\n",
    "References:\n",
    "* [Dask](https://docs.dask.org/en/latest/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure environment and create test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/home/hbabcock/Data/storm_analysis/jy_testing/\")\n",
    "print(os.getcwd())\n",
    "\n",
    "import numpy\n",
    "numpy.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import storm_analysis.diagnostics.daostorm_3d.settings as settings\n",
    "import storm_analysis.diagnostics.daostorm_3d.configure as configure\n",
    "import storm_analysis.diagnostics.daostorm_3d.make_data as makeData\n",
    "import storm_analysis.diagnostics.daostorm_3d.collate as collate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings.photons = [[10, 1000]]\n",
    "print(settings.photons)\n",
    "\n",
    "settings.iterations = 20\n",
    "settings.model = '2dfixed'\n",
    "settings.n_frames = 2000\n",
    "settings.peak_locations = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configure.configure()\n",
    "\n",
    "# You might want to change 'True' to 'False' if you are re-running\n",
    "# the notebook without changing the movie.\n",
    "if True:\n",
    "    makeData.makeData()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create job XML files\n",
    "\n",
    "#### Notes: \n",
    "\n",
    "* The number of divisions should be roughly the same as the number of workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import storm_analysis.sa_library.datareader as datareader\n",
    "\n",
    "import storm_analysis.slurm.check_analysis as checkAnalysis\n",
    "import storm_analysis.slurm.split_analysis_xml as splitAnalysisXML\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure out movie length.\n",
    "mv = datareader.inferReader(\"test_01/test.dax\")\n",
    "movie_len = mv.filmSize()[2]\n",
    "mv.close()\n",
    "\n",
    "# Make working directory.\n",
    "w_dir = \"test_01/work_dir\"\n",
    "if not os.path.exists(w_dir):\n",
    "    os.mkdir(w_dir)\n",
    "\n",
    "# Delete any existing XML files.\n",
    "for elt in checkAnalysis.getSortedJobXML(w_dir):\n",
    "    os.remove(elt)\n",
    "\n",
    "# Make job XML files. The number of divisions is the last argument\n",
    "# to this function. Typically you will get one extra division as the\n",
    "# first 10 frames are put into a single job as these tend to be\n",
    "# (at least for STORM imaging) quite dense.\n",
    "splitAnalysisXML.splitAnalysisXML(w_dir, \"dao.xml\", 0, movie_len, 4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dask analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "# Delete existing HDF5 files.\n",
    "for elt in glob.glob(os.path.join(w_dir, \"p*.hdf5\")):\n",
    "    os.remove(elt)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "\n",
    "import storm_analysis.daostorm_3d.mufit_analysis as mfit\n",
    "\n",
    "@dask.delayed\n",
    "def aJob(movie_name, mlist_name, xml_name):\n",
    "    mfit.analyze(movie_name, mlist_name, xml_name)\n",
    "    return 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create local client for distributed analysis.\n",
    "from dask.distributed import Client\n",
    "    \n",
    "# You may see errors if you specify multiple threads per worker.\n",
    "client = Client(threads_per_worker=1, n_workers=4)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find localizations in parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_xml_files = checkAnalysis.getSortedJobXML(w_dir)\n",
    "\n",
    "jobs = []\n",
    "\n",
    "m_name = os.path.abspath(\"test_01/test.dax\")\n",
    "\n",
    "# Assemble jobs.\n",
    "for i in range(len(job_xml_files)):\n",
    "    \n",
    "    h5_name = os.path.abspath(os.path.join(w_dir, \"p_{0:d}.hdf5\".format(i+1)))\n",
    "    xml_name = os.path.abspath(job_xml_files[i])\n",
    "    \n",
    "    a_job = aJob(m_name, h5_name, xml_name)\n",
    "    jobs.append(a_job)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run jobs.\n",
    "results = dask.compute(*jobs, schedule = 'distributed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close client.\n",
    "client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check and assemble results\n",
    "\n",
    "This checks that all of the HDF5 files were created and merges them all of them into a single HDF5 file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import storm_analysis.slurm.merge_analysis as mergeAnalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkAnalysis.checkAnalysis(w_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(\"test_01/test.hdf5\"):\n",
    "    os.remove(\"test_01/test.hdf5\")\n",
    "    \n",
    "mergeAnalysis.mergeAnalysis(w_dir, \"test_01/test.hdf5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the rest of analysis pipeline\n",
    "\n",
    "This will do the tracking, drift correction and z value checking steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import storm_analysis.sa_utilities.track_drift_correct as trackDriftCorrect\n",
    "\n",
    "trackDriftCorrect.trackDriftCorrect(\"test_01/test.hdf5\", \"dao.xml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
